{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPOnpCNUpFmEg7sBej308gl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/usernamehelloworld/18/blob/main/ollama_setup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xc68qxTRoar"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Install Dependencies (Run once per session)\n",
        "print(\"Installing necessary packages...\")\n",
        "!pip install colab-xterm gradio requests --quiet\n",
        "%load_ext colabxterm\n",
        "print(\"Packages installed and Xterm ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Download Config/Setup Files from GitHub (Run once per session)\n",
        "# Purpose: Downloads the Modelfile and the setup script from your GitHub repo.\n",
        "\n",
        "# !!! IMPORTANT: EDIT THIS LINE WITH YOUR GITHUB DETAILS !!!\n",
        "GITHUB_USERNAME=\"usernamehelloworld\" # Replace with your GitHub username\n",
        "GITHUB_REPONAME=\"UC\"                 # Replace with your GitHub repository name\n",
        "GITHUB_BRANCH=\"main\"               # Replace with your branch name if not 'main'\n",
        "# Construct the URL for raw file access\n",
        "GITHUB_REPO_URL = f\"https://raw.githubusercontent.com/{GITHUB_USERNAME}/{GITHUB_REPONAME}/{GITHUB_BRANCH}/\"\n",
        "\n",
        "MODELFILE_NAME = \"IncreaseContext.Modelfile\"\n",
        "SETUP_SCRIPT_NAME = \"setup_ollama.sh\"\n",
        "\n",
        "print(f\"Using GitHub URL: {GITHUB_REPO_URL}\")\n",
        "\n",
        "print(f\"Downloading {MODELFILE_NAME}...\")\n",
        "!curl -# -L -O {GITHUB_REPO_URL}{MODELFILE_NAME}\n",
        "\n",
        "print(f\"Downloading {SETUP_SCRIPT_NAME}...\")\n",
        "!curl -# -L -O {GITHUB_REPO_URL}{SETUP_SCRIPT_NAME}\n",
        "\n",
        "print(f\"Making {SETUP_SCRIPT_NAME} executable...\")\n",
        "!chmod +x {SETUP_SCRIPT_NAME}\n",
        "\n",
        "print(\"\\n--- Checking downloaded files ---\")\n",
        "!ls -l {MODELFILE_NAME} {SETUP_SCRIPT_NAME}\n",
        "print(\"---------------------------------\")\n",
        "\n",
        "print(f\"\\nFiles downloaded. Ready for Ollama Server Setup in %xterm using ./{SETUP_SCRIPT_NAME}\")\n",
        "print(\"NOTE: Pull any additional models you want to use via 'ollama pull model_name' in the first xterm after setup.\")"
      ],
      "metadata": {
        "id": "wmFrMQ26S125"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Open First %xterm for Ollama Server Setup (Run once per session)\n",
        "# Purpose: Opens the terminal where you will start the Ollama server.\n",
        "\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "print(\">>> Action Needed: Start Ollama Server <<<\")\n",
        "print(\"1. A terminal window (xterm) will open below.\")\n",
        "print(f\"2. Click inside the xterm window and type the command: ./setup_ollama.sh\")\n",
        "print(\"3. Press Enter to run the script.\")\n",
        "print(\"4. Wait for the script to complete (installs Ollama, starts server, creates model).\")\n",
        "print(\"5. OPTIONAL: Pull additional models using 'ollama pull model_name' (e.g., ollama pull llama3).\")\n",
        "print(\"6. IMPORTANT: Leave this xterm window OPEN. It keeps the Ollama server running.\")\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "%xterm"
      ],
      "metadata": {
        "id": "zwAdnaNFS9Y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Define Gradio App with Model Selection (Run once per session, AFTER server is running)\n",
        "# Purpose: Defines the Gradio UI and the Python functions to interact with Ollama.\n",
        "\n",
        "import gradio as gr\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "\n",
        "OLLAMA_API_BASE_URL = \"http://127.0.0.1:11434\" # Base URL for Ollama API\n",
        "DEFAULT_CUSTOM_MODEL = \"artifish/llama3.2-uncensored-8k:latest\" # The tag created by setup_ollama.sh\n",
        "\n",
        "# --- Helper Function to Get Available Ollama Models ---\n",
        "def get_ollama_models():\n",
        "    \"\"\"Fetches the list of locally available Ollama models via the API.\"\"\"\n",
        "    models = []\n",
        "    default_model = None\n",
        "    placeholder_message = \"Server Down / No Models Found\"\n",
        "    try:\n",
        "        print(\"Attempting to fetch models from Ollama server...\")\n",
        "        response = requests.get(f\"{OLLAMA_API_BASE_URL}/api/tags\", timeout=5) # Short timeout for check\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        if \"models\" in data and data[\"models\"]:\n",
        "            models = sorted([m[\"name\"] for m in data[\"models\"]]) # Sort alphabetically\n",
        "            print(f\"Found Ollama models: {models}\")\n",
        "            # Set default model\n",
        "            if DEFAULT_CUSTOM_MODEL in models:\n",
        "                 default_model = DEFAULT_CUSTOM_MODEL\n",
        "            elif models:\n",
        "                 default_model = models[0] # Fallback to first model\n",
        "            else:\n",
        "                 default_model = placeholder_message # Should not happen if models list has items\n",
        "        else:\n",
        "             print(\"Ollama server running, but no models found/pulled.\")\n",
        "             models = [placeholder_message]\n",
        "             default_model = placeholder_message\n",
        "\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        print(\"ERROR: Ollama server not reachable at {OLLAMA_API_BASE_URL}. Cannot get model list.\")\n",
        "        gr.Warning(\"Ollama server not running! Cannot fetch models. Please ensure Ollama is running (check the first xterm).\")\n",
        "        models = [placeholder_message]\n",
        "        default_model = placeholder_message\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching Ollama models: {e}\")\n",
        "        gr.Warning(f\"Error fetching Ollama models: {e}\")\n",
        "        models = [placeholder_message]\n",
        "        default_model = placeholder_message\n",
        "    return models, default_model\n",
        "\n",
        "# --- Modified Chat Function ---\n",
        "def ollama_chat_dynamic(model_name, message, history):\n",
        "    \"\"\" Talks to the selected Ollama model via API, streaming response. \"\"\"\n",
        "    if not model_name or model_name == \"Server Down / No Models Found\":\n",
        "        yield \"[Error: Please select a valid model from the dropdown and ensure the server is running.]\"\n",
        "        return\n",
        "\n",
        "    print(f\"Sending request to model: {model_name}\")\n",
        "    messages = []\n",
        "    for pair in history:\n",
        "        # Ensure history entries are valid before appending\n",
        "        if isinstance(pair, (list, tuple)) and len(pair) == 2:\n",
        "             messages.append({\"role\": \"user\", \"content\": str(pair[0]) if pair[0] is not None else \"\"})\n",
        "             messages.append({\"role\": \"assistant\", \"content\": str(pair[1]) if pair[1] is not None else \"\"})\n",
        "        else:\n",
        "             print(f\"Skipping invalid history entry: {pair}\")\n",
        "\n",
        "    messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    payload = {\n",
        "        \"model\": model_name,\n",
        "        \"messages\": messages,\n",
        "        \"stream\": True\n",
        "    }\n",
        "\n",
        "    full_response = \"\"\n",
        "    try:\n",
        "        # Use a session object for potential connection reuse\n",
        "        with requests.Session() as s:\n",
        "            with s.post(f\"{OLLAMA_API_BASE_URL}/api/chat\", json=payload, stream=True, timeout=300) as response: # 5 min timeout\n",
        "                response.raise_for_status()\n",
        "                print(\"Connection established, receiving stream...\")\n",
        "                for line in response.iter_lines():\n",
        "                    if line:\n",
        "                        try:\n",
        "                            data = json.loads(line.decode('utf-8'))\n",
        "                            if data.get(\"message\") and data[\"message\"].get(\"content\"):\n",
        "                                chunk = data[\"message\"][\"content\"]\n",
        "                                full_response += chunk\n",
        "                                yield full_response # Stream intermediate results\n",
        "                            if data.get(\"done\"):\n",
        "                                print(f\"Stream finished. Reason: {data.get('done_reason', 'N/A')}\")\n",
        "                                break\n",
        "                        except json.JSONDecodeError:\n",
        "                            print(f\"Warning: Could not decode JSON line: {line}\")\n",
        "                        except Exception as e:\n",
        "                             print(f\"Error processing stream line: {e}\")\n",
        "                             yield f\"[Error processing stream chunk: {e}]\" # Send error to UI\n",
        "                             return # Stop streaming on error\n",
        "                if not full_response:\n",
        "                     yield \"[Received empty response from model]\"\n",
        "\n",
        "    except requests.exceptions.Timeout:\n",
        "         print(\"ERROR: Request timed out.\")\n",
        "         yield \"[Error: Request to Ollama timed out. The model might be taking too long.]\"\n",
        "    except requests.exceptions.ConnectionError as e:\n",
        "        print(f\"ERROR connecting to Ollama: {e}\")\n",
        "        yield f\"[Error: Could not connect to Ollama server at {OLLAMA_API_BASE_URL}. Is it running?]\"\n",
        "    except requests.exceptions.RequestException as e:\n",
        "         print(f\"ERROR during Ollama request: {e}\")\n",
        "         yield f\"[Error during Ollama request: {e}]\"\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred in ollama_chat_dynamic: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc() # Print full traceback for debugging\n",
        "        yield f\"[An unexpected error occurred: {e}]\"\n",
        "\n",
        "\n",
        "# --- Build Gradio Interface using Blocks ---\n",
        "print(\"Defining Gradio Interface...\")\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# Ollama Chat Interface\")\n",
        "    gr.Markdown(\"Select a model you have pulled locally via Ollama. Ensure the Ollama server is running (check the first xterm).\")\n",
        "\n",
        "    with gr.Row():\n",
        "        # Fetch models when the UI is built\n",
        "        available_models_list, initial_default = get_ollama_models()\n",
        "        model_selector = gr.Dropdown(\n",
        "            label=\"Select Ollama Model\",\n",
        "            choices=available_models_list,\n",
        "            value=initial_default,\n",
        "            interactive=True # Allow user interaction\n",
        "        )\n",
        "        refresh_button = gr.Button(\"🔄 Refresh Models\")\n",
        "\n",
        "    chatbot = gr.Chatbot(label=\"Chat History\", height=550, show_copy_button=True)\n",
        "    msg = gr.Textbox(label=\"Your Message\", placeholder=\"Type your message and press Enter...\", scale=7) # Take more space\n",
        "    submit_btn = gr.Button(\"Send\", scale=1) # Explicit submit button\n",
        "    clear = gr.ClearButton([msg, chatbot], value=\"🗑️ Clear Chat\") # Use ClearButton\n",
        "\n",
        "    # Function to update the dropdown choices\n",
        "    def update_model_list():\n",
        "        print(\"Refreshing model list via button...\")\n",
        "        models, default = get_ollama_models()\n",
        "        # Update the dropdown component directly\n",
        "        return gr.Dropdown(choices=models, value=default)\n",
        "\n",
        "    # Link refresh button to update function\n",
        "    refresh_button.click(update_model_list, inputs=None, outputs=model_selector)\n",
        "\n",
        "    # Function to handle message submission (generator version for streaming)\n",
        "    def respond(model_name, message, chat_history):\n",
        "        if not message: # Do nothing if message is empty\n",
        "            return \"\", chat_history\n",
        "        if not model_name or model_name == \"Server Down / No Models Found\":\n",
        "             gr.Warning(\"Please select a valid model or refresh the list.\")\n",
        "             # Need to yield something to update UI state correctly\n",
        "             yield \"\", chat_history\n",
        "             return # Stop processing\n",
        "\n",
        "        # Append user message optimistically\n",
        "        chat_history.append([message, None]) # Add user message with None placeholder for bot response\n",
        "        # Yield immediately to show user message\n",
        "        yield \"\", chat_history\n",
        "\n",
        "        # Start streaming bot response\n",
        "        stream = ollama_chat_dynamic(model_name, message, chat_history[:-1]) # Pass history BEFORE adding the current user msg\n",
        "        bot_response = \"\"\n",
        "        for chunk in stream:\n",
        "            bot_response = chunk # Keep updating with the latest full response\n",
        "            chat_history[-1][1] = bot_response # Update the None placeholder\n",
        "            yield \"\", chat_history # Yield cleared input, updated history\n",
        "\n",
        "    # Link message submission (Enter key in Textbox OR Submit button click)\n",
        "    msg.submit(respond, [model_selector, msg, chatbot], [msg, chatbot])\n",
        "    submit_btn.click(respond, [model_selector, msg, chatbot], [msg, chatbot])\n",
        "\n",
        "print(\"Gradio Blocks defined.\")"
      ],
      "metadata": {
        "id": "UtYB5GYtTB6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Launch Gradio Interface (Run once per session, AFTER server is running)\n",
        "# Purpose: Starts the Gradio web UI.\n",
        "\n",
        "print(\"Launching Gradio Chat Interface...\")\n",
        "print(\"This might take a minute. Please wait for the public URL.\")\n",
        "\n",
        "# Launch the Gradio Blocks interface\n",
        "demo.launch(share=True, debug=True) # share=True provides a public link, debug=True shows logs\n",
        "\n",
        "print(\"Gradio app launched. Click the public URL link above to open the chat interface in a new tab.\")\n",
        "print(\"Remember to keep the first xterm window (running the Ollama server) open!\")"
      ],
      "metadata": {
        "id": "vgOhQYOnTD1Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}